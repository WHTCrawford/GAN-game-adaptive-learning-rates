{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the packages we are using, notably TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then specify some of the hyperparameters of the model. The structure of the learning is as follows. We randomly generate four lots of the parameter $\\phi$ and four lots of $\\gamma$. We then overwrite the first $\\phi$ with 0. This gives sixteen combinations of $\\phi$ and $\\gamma$, a quarter of which have $\\phi = 0$. This was done because $\\phi = 0$ corresponds to standard learning rates which we wanted to compare to game adaptive learning rates. Then for each step from 1 to __number_of_steps__ we generate __batch_size__ real samples and __batch_size__ fake samples and used these to carry out simulateneous minibatch gradient descent. In our experiments we iterated this process many times in order to collect over 10,000 parameter combinations in total, for simplicity we display the code for a single group of the sixteen parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_vec = np.random.uniform(0.0000001,0.1,4)\n",
    "phi_vec = np.random.uniform(0.0, 0.1, 4)\n",
    "phi_vec[0] = 0.0\n",
    "\n",
    "number_of_steps = 100000\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the mean and standard deviation of the Gaussian distribition which $G$ is attempting to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_mean = 6\n",
    "real_sd = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the discriminator and the generator, which are feed forward neural networks. $G$ has a single hidden layer with a tanh activation function, wheras $D$ has two hidden layers. Note we run the output of $D$ through the standard logistic function to constain it to be between 0 and 1. The hyperparameters __hidden_layer_size_d__ and __hidden_layer_size_g__ are unsurprisingly the number of nodes in the hidden layers of $D$ and $G$ respectively. Here we also define the weights and the biases which define the networks, which are all initialized to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size_d = 6\n",
    "hidden_layer_size_g = 5\n",
    "\n",
    "def discriminator(input, parameters):\n",
    "    pre_1 = tf.add(tf.matmul(tf.to_float(input), parameters[0]), \n",
    "                   parameters[1])\n",
    "    activ_1 = tf.tanh(pre_1)\n",
    "    pre_2 = tf.add(tf.matmul(activ_1, parameters[2]), \n",
    "                   parameters[3])\n",
    "    activ_2 = tf.tanh(pre_2)\n",
    "    pre_3 = tf.add(tf.matmul(activ_2, parameters[4]), \n",
    "                   parameters[5])\n",
    "    output = tf.sigmoid(pre_3)\n",
    "    return output\n",
    "\n",
    "\n",
    "def generator(input, parameters):\n",
    "    pre_1 = tf.add(tf.matmul(tf.to_float(input), parameters[0]), \n",
    "                   parameters[1])\n",
    "    activ_1 = tf.tanh(pre_1)\n",
    "    output = tf.add(tf.matmul(activ_1, parameters[2]), parameters[3])\n",
    "    return output\n",
    "\n",
    "weight_d_1 = tf.Variable(tf.random_uniform([1, hidden_layer_size_d], \n",
    "                            minval=-1, maxval=1, dtype=tf.float32))\n",
    "bias_d_1 = tf.Variable(tf.random_uniform([hidden_layer_size_d], \n",
    "                            minval=-1, maxval=1, dtype=tf.float32))\n",
    "weight_d_2 = tf.Variable(tf.random_uniform([hidden_layer_size_d, \n",
    "        hidden_layer_size_d], minval=-1, maxval=1, dtype=tf.float32))\n",
    "bias_d_2 = tf.Variable(tf.random_uniform([hidden_layer_size_d], \n",
    "                            minval=-1, maxval=1, dtype=tf.float32))\n",
    "weight_d_3 = tf.Variable(tf.random_uniform([hidden_layer_size_d, 1], \n",
    "                            minval=-1, maxval=1, dtype=tf.float32))\n",
    "bias_d_3 = tf.Variable(tf.random_uniform([1], minval=-1, maxval=1, \n",
    "                                                 dtype=tf.float32))\n",
    "\n",
    "d_parameters = [weight_d_1,bias_d_1, weight_d_2, bias_d_2,\n",
    "                weight_d_3, bias_d_3]\n",
    "\n",
    "weight_g_1 = tf.Variable(tf.random_uniform([1, hidden_layer_size_g], \n",
    "                            minval=-1, maxval=1, dtype=tf.float32))\n",
    "bias_g_1 = tf.Variable(tf.random_uniform([hidden_layer_size_g], \n",
    "                            minval=-1, maxval=1, dtype=tf.float32))\n",
    "weight_g_2 = tf.Variable(tf.random_uniform([hidden_layer_size_g, 1], \n",
    "                            minval=-1, maxval=1, dtype=tf.float32))\n",
    "bias_g_2 = tf.Variable(tf.random_uniform([1], minval=-1, maxval=1, \n",
    "                                                 dtype=tf.float32))\n",
    "\n",
    "g_parameters = [weight_g_1,bias_g_1, weight_g_2, bias_g_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the losses for $G$ and $D$. We first define place holders __real_dist_placeholder__ and __generator_input_placeholder__ which we will use to feed data into $G$ and $D$. We then define the output from $D$ when it is judging real and fake samples using the TensorFlow function __variable_scope.reuse_variables__ to ensure that the same parameters are used for the discriminator when it is given a real sample and a fake sample. The losses are as described in Section XYZ1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dist_placeholder = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "generator_input_placeholder = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "with tf.variable_scope(\"Discriminator\") as scope:\n",
    "    d_output_real = discriminator(real_dist_placeholder, d_parameters)\n",
    "    scope.reuse_variables()\n",
    "    d_output_fake = discriminator(generator(generator_input_placeholder, \n",
    "                                            g_parameters), d_parameters)\n",
    "loss_d = tf.reduce_mean(-tf.log(d_output_real) - tf.log(1 - d_output_fake))\n",
    "loss_g = tf.reduce_mean(tf.log(1-d_output_fake))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the nessescary parameters for game adaptive learning rates. Note we make sure the adjustment factor, __psi__, is less than zero and the empirical value function, __V__, is greater than zero to avoid any rounding errors. The formula for the adjustment factor, __psi__, can be found in Appendix XYZ2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_g = tf.placeholder(tf.float32)\n",
    "phi_d = tf.placeholder(tf.float32)\n",
    "gamma_g = tf.placeholder(tf.float32)\n",
    "gamma_d = tf.placeholder(tf.float32)\n",
    "psi_1 = tf.log((gamma_d - gamma_g - (2*phi_g) - phi_d)/\n",
    "               (-gamma_d+gamma_g-phi_d))/tf.log(16.0)\n",
    "psi = tf.maximum(0.0, psi_1)\n",
    "\n",
    "V = tf.minimum(tf.reduce_mean(tf.log(d_output_real)+\n",
    "                              tf.log(1-d_output_fake)),0)\n",
    "\n",
    "learning_rate_d = gamma_d-phi_d*tf.tanh(psi*V)\n",
    "learning_rate_g = gamma_g + phi_g*(1 + tf.tanh(psi*V))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the training steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_g = tf.train.GradientDescentOptimizer(learning_rate_g).\\\n",
    "                        minimize(loss_g, var_list=g_parameters)\n",
    "train_d = tf.train.GradientDescentOptimizer(learning_rate_d).\\\n",
    "                        minimize(loss_d, var_list=d_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training iterations we will save the final generated distribution to file in order to anaylse. Therefore we create a matrix to store these reuslts in. Recall __gamma_vec__ and __phi_vec__ hold the values of $\\phi$ and $\\gamma$ we will be using in the sixteen simulations. In each combination of $\\phi$ and $\\gamma$ we first open a TensorFlow session and initalize the variables. We then perform the learning steps. Finally we create a generated batch to export. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_matrix = np.zeros((len(gamma_vec) * len(phi_vec), batch_size))\n",
    "gamma_out_vec, phi_out_vec = np.zeros((len(gamma_vec) * len(phi_vec))),\\\n",
    "np.zeros((len(gamma_vec) * len(phi_vec)))\n",
    "\n",
    "row =0 # This indexes which row of res_matrix we are writing to.\n",
    "for i, p in enumerate(phi_vec):\n",
    "    for j, k in enumerate(gamma_vec):\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            for step in range(1, number_of_steps+1):\n",
    "                generator_input = np.random.uniform(0, 1, \n",
    "                                                    (batch_size, 1))\n",
    "                real_dist = np.random.normal(real_mean, real_sd, \n",
    "                                                     (batch_size, 1))\n",
    "\n",
    "                sess.run(train_d, feed_dict={real_dist_placeholder: \n",
    "                                             real_dist,\n",
    "                                             generator_input_placeholder: \n",
    "                                             generator_input, \n",
    "                                             phi_g: p,phi_d:p, \n",
    "                                             gamma_g:k,gamma_d:k })\n",
    "                sess.run(train_g, feed_dict={real_dist_placeholder: \n",
    "                                             real_dist,\n",
    "                                             generator_input_placeholder: \n",
    "                                             generator_input, \n",
    "                                             phi_g: p,phi_d:p, \n",
    "                                             gamma_g:k,gamma_d:k })\n",
    "                \n",
    "            generator_input = np.random.uniform(0, 1, (batch_size, 1))\n",
    "            real_dist = np.random.normal(real_mean, real_sd, (batch_size, 1))\n",
    "\n",
    "            generated = sess.run(generator(generator_input,g_parameters))\n",
    "            res_matrix[row] = generated.reshape(batch_size)\n",
    "            gamma_out_vec[row] = k\n",
    "            phi_out_vec[row] = p\n",
    "            row = row + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have iterated through these sixteen $(\\phi, \\gamma)$ combinations we save the ouput to file, the resulting data has $\\gamma$ in the first column, $\\phi$ in the second and then the 1000 generated samples in columns 3 to 1002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dataframe = pd.DataFrame(data=res_matrix.astype(float))\n",
    "gamma_dataframe = pd.DataFrame(data=gamma_out_vec.astype(float))\n",
    "phi_dataframe = pd.DataFrame(data=phi_out_vec.astype(float))\n",
    "\n",
    "output_dataframe1 = pd.concat([gamma_dataframe.reset_index(drop=True), \n",
    "                               phi_dataframe], axis=1)\n",
    "output_dataframe2 = pd.concat([output_dataframe1.reset_index(drop=True), \n",
    "                               res_dataframe], axis=1)\n",
    "\n",
    "with open(\"output.csv\", 'a') as f:\n",
    "    output_dataframe2.to_csv(f, sep=',', header=False, float_format='%.9f', \n",
    "                             index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
